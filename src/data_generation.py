"""
Module: data_generation.py
Objectif: GÃ©nÃ©rer des Ã©tats quantiques valides et invalides
Auteur: [Ton nom]
Date: 2024-11-12

Ce module contient les fonctions pour crÃ©er des datasets d'Ã©tats quantiques
avec diffÃ©rentes stratÃ©gies de gÃ©nÃ©ration.
"""

import numpy as np
from typing import Tuple, Optional
import warnings


def generate_valid_states(
    n_samples: int,
    dim: int,
    strategy: str = "random",
    alpha: float = 1.0,
    seed: Optional[int] = None,
) -> np.ndarray:
    """
    GÃ©nÃ¨re des Ã©tats quantiques valides (normalisÃ©s).

    ParamÃ¨tres
    ----------
    n_samples : int
        Nombre d'Ã©tats Ã  gÃ©nÃ©rer.

    dim : int
        Dimension de l'espace de Hilbert (nombre de composantes).
        Exemples: dim=2 (qubit), dim=3 (qutrit), dim=4, etc.

    strategy : str, optional
        StratÃ©gie de gÃ©nÃ©ration:
        - "random" : gÃ©nÃ©ration gaussienne + normalisation (dÃ©faut)
        - "dirichlet" : distribution de Dirichlet pour les probabilitÃ©s
        - "basis" : Ã©tats purs de la base canonique

    alpha : float, optional
        ParamÃ¨tre de concentration pour stratÃ©gie "dirichlet".
        - alpha = 1.0 : uniforme (dÃ©faut)
        - alpha > 1.0 : favorise probabilitÃ©s Ã©quilibrÃ©es
        - alpha < 1.0 : favorise probabilitÃ©s dÃ©sÃ©quilibrÃ©es

    seed : int, optional
        Graine alÃ©atoire pour reproductibilitÃ©.
        Si None, utilise l'Ã©tat alÃ©atoire actuel de NumPy.

    Retourne
    --------
    states : np.ndarray
        Tableau de shape (n_samples, dim) contenant les Ã©tats gÃ©nÃ©rÃ©s.
        Chaque ligne est un Ã©tat quantique normalisÃ© (dtype=complex128).

    Raises
    ------
    ValueError
        Si strategy n'est pas reconnue.
        Si n_samples <= 0 ou dim <= 0.

    Exemples
    --------
    >>> # GÃ©nÃ©rer 100 qubits avec stratÃ©gie random
    >>> states = generate_valid_states(100, dim=2, strategy="random", seed=42)
    >>> states.shape
    (100, 2)
    >>> np.allclose(np.sum(np.abs(states)**2, axis=1), 1.0)
    True

    Notes
    -----
    Toutes les stratÃ©gies garantissent que ||Ïˆ||Â² = 1 pour chaque Ã©tat.
    """

    # === Validation des paramÃ¨tres ===
    if n_samples <= 0:
        raise ValueError(f"n_samples doit Ãªtre > 0, reÃ§u: {n_samples}")

    if dim <= 0:
        raise ValueError(f"dim doit Ãªtre > 0, reÃ§u: {dim}")

    if strategy not in ["random", "dirichlet", "basis"]:
        raise ValueError(
            f"strategy '{strategy}' inconnue. "
            f"Choix possibles: 'random', 'dirichlet', 'basis'"
        )

    # === Initialisation du gÃ©nÃ©rateur alÃ©atoire ===
    rng = np.random.default_rng(seed)  # GÃ©nÃ©rateur moderne NumPy

    # === GÃ©nÃ©ration selon la stratÃ©gie choisie ===

    if strategy == "random":
        # StratÃ©gie 1 : GÃ©nÃ©ration gaussienne + normalisation

        # GÃ©nÃ¨re parties rÃ©elles et imaginaires indÃ©pendantes
        # Distribution normale centrÃ©e rÃ©duite N(0, 1)
        real_parts = rng.normal(loc=0.0, scale=1.0, size=(n_samples, dim))
        imag_parts = rng.normal(loc=0.0, scale=1.0, size=(n_samples, dim))

        # Construit les coefficients complexes
        states = real_parts + 1j * imag_parts

        # Normalise chaque Ã©tat
        # norms: shape (n_samples,) contenant ||Ïˆ||Â² pour chaque Ã©tat
        norms = np.sqrt(np.sum(np.abs(states) ** 2, axis=1, keepdims=True))

        # Ã‰vite division par zÃ©ro (cas extrÃªmement rare)
        norms = np.where(norms == 0, 1.0, norms)

        states = states / norms

    elif strategy == "dirichlet":
        # StratÃ©gie 2 : Distribution de Dirichlet pour les probabilitÃ©s

        # GÃ©nÃ¨re les probabilitÃ©s via Dirichlet
        # alpha_vec: vecteur de paramÃ¨tres de concentration
        alpha_vec = np.full(dim, alpha)

        # probabilities: shape (n_samples, dim)
        # Chaque ligne somme Ã  1.0
        probabilities = rng.dirichlet(alpha_vec, size=n_samples)

        # GÃ©nÃ¨re des phases alÃ©atoires uniformes dans [0, 2Ï€]
        phases = rng.uniform(0, 2 * np.pi, size=(n_samples, dim))

        # Construit les coefficients complexes
        # c_i = âˆšp_i Â· e^(iÏ†_i) = âˆšp_i Â· (cos(Ï†_i) + iÂ·sin(Ï†_i))
        amplitudes = np.sqrt(probabilities)
        states = amplitudes * np.exp(1j * phases)

        # VÃ©rification (devrait dÃ©jÃ  Ãªtre normalisÃ© par construction)
        # Mais on normalise quand mÃªme pour Ã©viter erreurs numÃ©riques
        norms = np.sqrt(np.sum(np.abs(states) ** 2, axis=1, keepdims=True))
        states = states / norms

    elif strategy == "basis":
        # StratÃ©gie 3 : Ã‰tats purs de la base canonique

        # Si n_samples > dim, on gÃ©nÃ¨re plusieurs copies de chaque Ã©tat
        states = np.zeros((n_samples, dim), dtype=complex)

        for i in range(n_samples):
            # SÃ©lectionne un indice de base alÃ©atoirement
            basis_index = rng.integers(0, dim)

            # CrÃ©e l'Ã©tat pur |basis_indexâŸ©
            states[i, basis_index] = 1.0 + 0j

    # === VÃ©rification finale (optionnelle, pour debug) ===
    # DÃ©commente pour vÃ©rifier que tous les Ã©tats sont bien normalisÃ©s
    # norms_check = np.sum(np.abs(states)**2, axis=1)
    # assert np.allclose(norms_check, 1.0), "Certains Ã©tats ne sont pas normalisÃ©s!"

    return states


def verify_normalization(
    states: np.ndarray, tolerance: float = 1e-6
) -> Tuple[bool, np.ndarray]:
    """
    VÃ©rifie que tous les Ã©tats d'un batch sont normalisÃ©s.

    ParamÃ¨tres
    ----------
    states : np.ndarray
        Tableau de shape (n_samples, dim) contenant les Ã©tats.

    tolerance : float, optional
        TolÃ©rance numÃ©rique pour la vÃ©rification.

    Retourne
    --------
    all_valid : bool
        True si TOUS les Ã©tats sont normalisÃ©s, False sinon.

    norms_squared : np.ndarray
        Tableau de shape (n_samples,) contenant ||Ïˆ||Â² pour chaque Ã©tat.

    Exemples
    --------
    >>> states = generate_valid_states(10, dim=3, seed=42)
    >>> all_valid, norms = verify_normalization(states)
    >>> all_valid
    True
    >>> np.allclose(norms, 1.0)
    True
    """

    # Calcule ||Ïˆ||Â² pour chaque Ã©tat
    norms_squared = np.sum(np.abs(states) ** 2, axis=1)

    # VÃ©rifie si tous sont proches de 1.0
    all_valid = np.allclose(norms_squared, 1.0, atol=tolerance)

    return all_valid, norms_squared


# === Fonctions utilitaires ===


def get_strategy_info() -> dict:
    """
    Retourne un dictionnaire dÃ©crivant les stratÃ©gies disponibles.

    Retourne
    --------
    info : dict
        Dictionnaire {strategy_name: description}.
    """

    info = {
        "random": (
            "GÃ©nÃ©ration gaussienne + normalisation. "
            "Explore uniformÃ©ment l'espace des Ã©tats quantiques. "
            "RecommandÃ© pour usage gÃ©nÃ©ral."
        ),
        "dirichlet": (
            "Distribution de Dirichlet pour les probabilitÃ©s. "
            "ParamÃ¨tre alpha contrÃ´le la dispersion: "
            "alpha=1 (uniforme), alpha>1 (Ã©quilibrÃ©), alpha<1 (pics). "
            "Utile pour tester diffÃ©rentes distributions de probabilitÃ©s."
        ),
        "basis": (
            "Ã‰tats purs de la base canonique. "
            "GÃ©nÃ¨re des Ã©tats du type |0âŸ©, |1âŸ©, ..., |n-1âŸ©. "
            "Utile pour avoir des cas triviaux dans le dataset."
        ),
    }

    return info


def print_strategy_info():
    """
    Affiche les informations sur les stratÃ©gies de gÃ©nÃ©ration.
    """
    info = get_strategy_info()

    print("=" * 70)
    print("STRATÃ‰GIES DE GÃ‰NÃ‰RATION D'Ã‰TATS VALIDES")
    print("=" * 70)

    for strategy, description in info.items():
        print(f"\nğŸ“Œ {strategy.upper()}")
        print(f"   {description}")

    print("\n" + "=" * 70)


# === Exemple d'utilisation (si le script est exÃ©cutÃ© directement) ===

# ============================================================================
# GÃ‰NÃ‰RATION D'Ã‰TATS INVALIDES (NON NORMALISÃ‰S)
# ============================================================================


def generate_invalid_states(
    n_samples: int,
    dim: int,
    strategy: str = "scaling",
    scale_range: Tuple[float, float] = (0.1, 2.0),
    noise_level: float = 0.3,
    extreme_prob: float = 0.1,
    seed: Optional[int] = None,
) -> np.ndarray:
    """
    GÃ©nÃ¨re des Ã©tats quantiques invalides (non normalisÃ©s).

    ParamÃ¨tres
    ----------
    n_samples : int
        Nombre d'Ã©tats invalides Ã  gÃ©nÃ©rer.

    dim : int
        Dimension de l'espace de Hilbert.

    strategy : str, optional
        StratÃ©gie de gÃ©nÃ©ration:
        - "scaling" : multiplie des Ã©tats valides par un facteur k â‰  1 (dÃ©faut)
        - "noise" : ajoute du bruit Ã  des Ã©tats valides sans renormaliser
        - "direct" : gÃ©nÃ¨re directement sans normalisation
        - "mixed" : mÃ©lange des 3 stratÃ©gies + cas extrÃªmes

    scale_range : tuple of float, optional
        Pour stratÃ©gie "scaling": intervalle [k_min, k_max] pour le facteur k.
        Par dÃ©faut: (0.1, 2.0) en Ã©vitant [0.95, 1.05] pour Ã©viter ambiguÃ¯tÃ©.

    noise_level : float, optional
        Pour stratÃ©gie "noise": intensitÃ© du bruit (epsilon).
        Par dÃ©faut: 0.3

    extreme_prob : float, optional
        Pour stratÃ©gie "mixed": probabilitÃ© de gÃ©nÃ©rer un cas extrÃªme.
        Par dÃ©faut: 0.1 (10% de cas extrÃªmes)

    seed : int, optional
        Graine alÃ©atoire pour reproductibilitÃ©.

    Retourne
    --------
    states : np.ndarray
        Tableau de shape (n_samples, dim) contenant les Ã©tats invalides.
        dtype=complex128.
        Garantie: AUCUN Ã©tat n'est normalisÃ© (||Ïˆ||Â² â‰  1).

    Raises
    ------
    ValueError
        Si strategy n'est pas reconnue.

    Exemples
    --------
    >>> states = generate_invalid_states(100, dim=3, strategy="scaling", seed=42)
    >>> all_valid, norms = verify_normalization(states)
    >>> all_valid
    False
    >>> (norms != 1.0).all()
    True

    Notes
    -----
    Pour stratÃ©gie "scaling", on Ã©vite k âˆˆ [0.95, 1.05] pour crÃ©er une
    sÃ©paration claire entre Ã©tats valides et invalides.
    """

    # Validation
    if n_samples <= 0:
        raise ValueError(f"n_samples doit Ãªtre > 0, reÃ§u: {n_samples}")

    if dim <= 0:
        raise ValueError(f"dim doit Ãªtre > 0, reÃ§u: {dim}")

    valid_strategies = ["scaling", "noise", "direct", "mixed"]
    if strategy not in valid_strategies:
        raise ValueError(
            f"strategy '{strategy}' inconnue. " f"Choix possibles: {valid_strategies}"
        )

    rng = np.random.default_rng(seed)

    # === STRATÃ‰GIE SCALING ===
    if strategy == "scaling":
        # GÃ©nÃ¨re d'abord des Ã©tats valides
        states_valid = generate_valid_states(
            n_samples=n_samples,
            dim=dim,
            strategy="random",
            seed=rng.integers(0, 1e9),  # Seed alÃ©atoire diffÃ©rent
        )

        # GÃ©nÃ¨re des facteurs de scaling k
        # On Ã©vite l'intervalle [0.95, 1.05] pour Ã©viter ambiguÃ¯tÃ©
        k_min, k_max = scale_range

        # GÃ©nÃ¨re k uniformÃ©ment dans [k_min, k_max]
        factors = rng.uniform(k_min, k_max, size=n_samples)

        # Exclut l'intervalle [0.95, 1.05]
        # Si k tombe dans cet intervalle, on le repousse
        mask_ambiguous = (factors >= 0.95) & (factors <= 1.05)
        n_ambiguous = mask_ambiguous.sum()

        if n_ambiguous > 0:
            # Remplace les valeurs ambiguÃ«s par des valeurs claires
            # 50% en dessous de 0.95, 50% au-dessus de 1.05
            new_factors = np.where(
                rng.random(n_ambiguous) < 0.5,
                rng.uniform(k_min, 0.95, size=n_ambiguous),
                rng.uniform(1.05, k_max, size=n_ambiguous),
            )
            factors[mask_ambiguous] = new_factors

        # Applique le scaling
        # Broadcasting: (n_samples,) Ã— (n_samples, dim)
        states = states_valid * factors[:, np.newaxis]

    # === STRATÃ‰GIE NOISE ===
    elif strategy == "noise":
        # GÃ©nÃ¨re des Ã©tats valides
        states_valid = generate_valid_states(
            n_samples=n_samples, dim=dim, strategy="random", seed=rng.integers(0, 1e9)
        )

        # GÃ©nÃ¨re du bruit complexe
        noise_real = rng.normal(0, noise_level, size=(n_samples, dim))
        noise_imag = rng.normal(0, noise_level, size=(n_samples, dim))
        noise = noise_real + 1j * noise_imag

        # Ajoute le bruit (sans renormaliser !)
        states = states_valid + noise

    # === STRATÃ‰GIE DIRECT ===
    elif strategy == "direct":
        # GÃ©nÃ¨re directement sans normaliser
        real_parts = rng.normal(0, 1, size=(n_samples, dim))
        imag_parts = rng.normal(0, 1, size=(n_samples, dim))
        states = real_parts + 1j * imag_parts

        # Applique un scaling alÃ©atoire pour varier ||Ïˆ||Â²
        scale_factors = rng.uniform(0.1, 3.0, size=n_samples)
        states = states * scale_factors[:, np.newaxis]

    # === STRATÃ‰GIE MIXED ===
    elif strategy == "mixed":
        states = np.zeros((n_samples, dim), dtype=complex)

        # RÃ©partition des sous-stratÃ©gies
        n_extreme = int(n_samples * extreme_prob)
        n_remaining = n_samples - n_extreme

        # Distribution du reste entre scaling, noise, direct
        n_scaling = n_remaining // 3
        n_noise = n_remaining // 3
        n_direct = n_remaining - n_scaling - n_noise

        idx = 0

        # 1. Cas extrÃªmes
        if n_extreme > 0:
            states_extreme = _generate_extreme_states(n_extreme, dim, rng)
            states[idx : idx + n_extreme] = states_extreme
            idx += n_extreme

        # 2. Scaling
        if n_scaling > 0:
            states_scaling = generate_invalid_states(
                n_scaling,
                dim,
                strategy="scaling",
                scale_range=scale_range,
                seed=rng.integers(0, 1e9),
            )
            states[idx : idx + n_scaling] = states_scaling
            idx += n_scaling

        # 3. Noise
        if n_noise > 0:
            states_noise = generate_invalid_states(
                n_noise,
                dim,
                strategy="noise",
                noise_level=noise_level,
                seed=rng.integers(0, 1e9),
            )
            states[idx : idx + n_noise] = states_noise
            idx += n_noise

        # 4. Direct
        if n_direct > 0:
            states_direct = generate_invalid_states(
                n_direct, dim, strategy="direct", seed=rng.integers(0, 1e9)
            )
            states[idx : idx + n_direct] = states_direct

        # MÃ©lange alÃ©atoirement
        rng.shuffle(states)

    # === VÃ‰RIFICATION FINALE ===
    # S'assure qu'aucun Ã©tat n'est accidentellement normalisÃ©
    norms_squared = np.sum(np.abs(states) ** 2, axis=1)
    accidentally_normalized = np.isclose(norms_squared, 1.0, atol=1e-4)

    if accidentally_normalized.any():
        # Rescale lÃ©gÃ¨rement ces Ã©tats
        indices = np.where(accidentally_normalized)[0]
        for idx in indices:
            # Multiplie par un facteur alÃ©atoire loin de 1
            factor = rng.choice([0.7, 0.8, 1.2, 1.3])
            states[idx] *= factor

    return states


def _generate_extreme_states(n_samples: int, dim: int, rng) -> np.ndarray:
    """
    GÃ©nÃ¨re des cas extrÃªmes (outliers) pour tester la robustesse.

    Cas gÃ©nÃ©rÃ©s:
    - Ã‰tats nuls ou quasi-nuls (||Ïˆ||Â² â‰ˆ 0)
    - Ã‰tats trÃ¨s grands (||Ïˆ||Â² >> 1)
    - Ã‰tats avec une composante dominante Ã©norme

    Fonction interne, pas destinÃ©e Ã  Ãªtre utilisÃ©e directement.
    """
    states = np.zeros((n_samples, dim), dtype=complex)

    for i in range(n_samples):
        case_type = rng.choice(["null", "huge", "unbalanced"])

        if case_type == "null":
            # Ã‰tat quasi-nul
            states[i] = rng.normal(0, 0.01, dim) + 1j * rng.normal(0, 0.01, dim)

        elif case_type == "huge":
            # Ã‰tat trÃ¨s grand
            states[i] = rng.normal(10, 5, dim) + 1j * rng.normal(10, 5, dim)

        elif case_type == "unbalanced":
            # Une composante Ã©norme, les autres petites
            dominant_idx = rng.integers(0, dim)
            states[i] = rng.normal(0, 0.1, dim) + 1j * rng.normal(0, 0.1, dim)
            states[i, dominant_idx] = rng.uniform(50, 100) + 1j * rng.uniform(50, 100)

    return states


def get_invalid_strategy_info() -> dict:
    """
    Retourne les descriptions des stratÃ©gies de gÃ©nÃ©ration d'Ã©tats invalides.
    """
    info = {
        "scaling": (
            "Multiplie des Ã©tats valides par un facteur k â‰  1. "
            "ContrÃ´le: k âˆˆ [k_min, k_max] en Ã©vitant [0.95, 1.05]. "
            "Produit: ||Ïˆ||Â² = kÂ². "
            "RecommandÃ© pour usage gÃ©nÃ©ral."
        ),
        "noise": (
            "Ajoute du bruit Ã  des Ã©tats valides sans renormaliser. "
            "ParamÃ¨tre noise_level contrÃ´le l'intensitÃ©. "
            "Produit: Ã©tats 'presque valides' (utile pour robustesse)."
        ),
        "direct": (
            "GÃ©nÃ¨re directement des coefficients sans normalisation. "
            "Produit: large distribution de ||Ïˆ||Â². "
            "Bonne diversitÃ©."
        ),
        "mixed": (
            "Combine scaling, noise, direct + cas extrÃªmes. "
            "ParamÃ¨tre extreme_prob contrÃ´le % de outliers. "
            "Produit: dataset trÃ¨s diversifiÃ©. "
            "RecommandÃ© pour dataset final."
        ),
    }
    return info


def print_invalid_strategy_info():
    """
    Affiche les informations sur les stratÃ©gies d'Ã©tats invalides.
    """
    info = get_invalid_strategy_info()

    print("=" * 70)
    print("STRATÃ‰GIES DE GÃ‰NÃ‰RATION D'Ã‰TATS INVALIDES")
    print("=" * 70)

    for strategy, description in info.items():
        print(f"\nğŸ“Œ {strategy.upper()}")
        print(f"   {description}")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    # Ce bloc s'exÃ©cute uniquement si on lance: python src/data_generation.py

    print("Test du module data_generation.py\n")

    # Affiche les stratÃ©gies disponibles
    print_strategy_info()

    # Test des 3 stratÃ©gies
    print("\n" + "=" * 70)
    print("TESTS DE GÃ‰NÃ‰RATION")
    print("=" * 70)

    dim = 3
    n_samples = 5

    for strategy in ["random", "dirichlet", "basis"]:
        print(f"\n--- StratÃ©gie: {strategy} ---")

        states = generate_valid_states(
            n_samples=n_samples, dim=dim, strategy=strategy, seed=42
        )

        print(f"Shape: {states.shape}")
        print(f"Dtype: {states.dtype}")

        # VÃ©rification
        all_valid, norms = verify_normalization(states)
        print(f"Tous normalisÃ©s? {all_valid}")
        print(f"NormesÂ²: {norms}")

        # Affiche les 2 premiers Ã©tats
        print(f"\n2 premiers Ã©tats:")
        for i in range(min(2, n_samples)):
            print(f"  Ã‰tat {i}: {states[i]}")
            print(f"    ||Ïˆ||Â² = {norms[i]:.10f}")
