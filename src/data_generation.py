"""
Module: data_generation.py
Objectif: G√©n√©rer des √©tats quantiques valides et invalides
Auteur: Mkl Zenin
Date: 2024-11-12

Ce module contient les fonctions pour cr√©er des datasets d'√©tats quantiques avec diff√©rentes strat√©gies de g√©n√©ration.
"""

import numpy as np
from typing import Tuple, Optional
import warnings
from pathlib import Path


def generate_valid_states(
    n_samples: int,
    dim: int,
    strategy: str = "random",
    alpha: float = 1.0,
    seed: Optional[int] = None,
) -> np.ndarray:

    # === Validation des param√®tres ===
    if n_samples <= 0:
        raise ValueError(f"n_samples doit √™tre > 0, re√ßu: {n_samples}")

    if dim <= 0:
        raise ValueError(f"dim doit √™tre > 0, re√ßu: {dim}")

    if strategy not in ["random", "dirichlet", "basis"]:
        raise ValueError(
            f"strategy '{strategy}' inconnue. "
            f"Choix possibles: 'random', 'dirichlet', 'basis'"
        )

    # === Initialisation du g√©n√©rateur al√©atoire ===
    rng = np.random.default_rng(seed)  # G√©n√©rateur moderne NumPy

    # === G√©n√©ration selon la strat√©gie choisie ===

    if strategy == "random":
        # Strat√©gie 1 : G√©n√©ration gaussienne + normalisation

        # G√©n√®re parties r√©elles et imaginaires ind√©pendantes
        # Distribution normale centr√©e r√©duite N(0, 1)
        real_parts = rng.normal(loc=0.0, scale=1.0, size=(n_samples, dim))
        imag_parts = rng.normal(loc=0.0, scale=1.0, size=(n_samples, dim))

        # Construit les coefficients complexes
        states = real_parts + 1j * imag_parts

        # Normalise chaque √©tat
        # norms: shape (n_samples,) contenant ||œà||¬≤ pour chaque √©tat
        norms = np.sqrt(np.sum(np.abs(states) ** 2, axis=1, keepdims=True))

        # √âvite division par z√©ro (cas extr√™mement rare)
        norms = np.where(norms == 0, 1.0, norms)

        states = states / norms

    elif strategy == "dirichlet":
        # Strat√©gie 2 : Distribution de Dirichlet pour les probabilit√©s

        # G√©n√®re les probabilit√©s via Dirichlet
        # alpha_vec: vecteur de param√®tres de concentration
        alpha_vec = np.full(dim, alpha)

        # probabilities: shape (n_samples, dim)
        # Chaque ligne somme √† 1.0
        probabilities = rng.dirichlet(alpha_vec, size=n_samples)

        # G√©n√®re des phases al√©atoires uniformes dans [0, 2œÄ]
        phases = rng.uniform(0, 2 * np.pi, size=(n_samples, dim))

        # Construit les coefficients complexes
        # c_i = ‚àöp_i ¬∑ e^(iœÜ_i) = ‚àöp_i ¬∑ (cos(œÜ_i) + i¬∑sin(œÜ_i))
        amplitudes = np.sqrt(probabilities)
        states = amplitudes * np.exp(1j * phases)

        # V√©rification (devrait d√©j√† √™tre normalis√© par construction)
        # Mais on normalise quand m√™me pour √©viter erreurs num√©riques
        norms = np.sqrt(np.sum(np.abs(states) ** 2, axis=1, keepdims=True))
        states = states / norms

    elif strategy == "basis":
        # Strat√©gie 3 : √âtats purs de la base canonique

        # Si n_samples > dim, on g√©n√®re plusieurs copies de chaque √©tat
        states = np.zeros((n_samples, dim), dtype=complex)

        for i in range(n_samples):
            # S√©lectionne un indice de base al√©atoirement
            basis_index = rng.integers(0, dim)

            # Cr√©e l'√©tat pur |basis_index‚ü©
            states[i, basis_index] = 1.0 + 0j

    # === V√©rification finale (optionnelle, pour debug) ===
    # D√©commente pour v√©rifier que tous les √©tats sont bien normalis√©s
    # norms_check = np.sum(np.abs(states)**2, axis=1)
    # assert np.allclose(norms_check, 1.0), "Certains √©tats ne sont pas normalis√©s!"

    return states


def verify_normalization(
    states: np.ndarray, tolerance: float = 1e-6
) -> Tuple[bool, np.ndarray]:

    # Calcule ||œà||¬≤ pour chaque √©tat
    norms_squared = np.sum(np.abs(states) ** 2, axis=1)

    # V√©rifie si tous sont proches de 1.0
    all_valid = np.allclose(norms_squared, 1.0, atol=tolerance)

    return all_valid, norms_squared


# === Fonctions utilitaires ===


def get_strategy_info() -> dict:
    """
    Retourne un dictionnaire d√©crivant les strat√©gies disponibles.

    Retourne
    --------
    info : dict
        Dictionnaire {strategy_name: description}.
    """

    info = {
        "random": (
            "G√©n√©ration gaussienne + normalisation. "
            "Explore uniform√©ment l'espace des √©tats quantiques. "
            "Recommand√© pour usage g√©n√©ral."
        ),
        "dirichlet": (
            "Distribution de Dirichlet pour les probabilit√©s. "
            "Param√®tre alpha contr√¥le la dispersion: "
            "alpha=1 (uniforme), alpha>1 (√©quilibr√©), alpha<1 (pics). "
            "Utile pour tester diff√©rentes distributions de probabilit√©s."
        ),
        "basis": (
            "√âtats purs de la base canonique. "
            "G√©n√®re des √©tats du type |0‚ü©, |1‚ü©, ..., |n-1‚ü©. "
            "Utile pour avoir des cas triviaux dans le dataset."
        ),
    }

    return info


def print_strategy_info():
    """
    Affiche les informations sur les strat√©gies de g√©n√©ration.
    """
    info = get_strategy_info()

    print("=" * 70)
    print("STRAT√âGIES DE G√âN√âRATION D'√âTATS VALIDES")
    print("=" * 70)

    for strategy, description in info.items():
        print(f"\n {strategy.upper()}")
        print(f"   {description}")

    print("\n" + "=" * 70)


# === Exemple d'utilisation (si le script est ex√©cut√© directement) ===

# ============================================================================
# G√âN√âRATION D'√âTATS INVALIDES (NON NORMALIS√âS)
# ============================================================================


def generate_invalid_states(
    n_samples: int,
    dim: int,
    strategy: str = "scaling",
    scale_range: Tuple[float, float] = (0.1, 2.0),
    noise_level: float = 0.3,
    extreme_prob: float = 0.1,
    seed: Optional[int] = None,
) -> np.ndarray:

    # Validation
    if n_samples <= 0:
        raise ValueError(f"n_samples doit √™tre > 0, re√ßu: {n_samples}")

    if dim <= 0:
        raise ValueError(f"dim doit √™tre > 0, re√ßu: {dim}")

    valid_strategies = ["scaling", "noise", "direct", "mixed"]
    if strategy not in valid_strategies:
        raise ValueError(
            f"strategy '{strategy}' inconnue. " f"Choix possibles: {valid_strategies}"
        )

    rng = np.random.default_rng(seed)

    # === STRAT√âGIE SCALING ===
    if strategy == "scaling":
        # G√©n√®re d'abord des √©tats valides
        states_valid = generate_valid_states(
            n_samples=n_samples,
            dim=dim,
            strategy="random",
            seed=int(
                rng.integers(0, int(1e9))
            ),  # Seed al√©atoire diff√©rent (cast to Python int)
        )

        # G√©n√®re des facteurs de scaling k
        # On √©vite l'intervalle [0.95, 1.05] pour √©viter ambigu√Øt√©
        k_min, k_max = scale_range

        # G√©n√®re k uniform√©ment dans [k_min, k_max]
        factors = rng.uniform(k_min, k_max, size=n_samples)

        # Exclut l'intervalle [0.95, 1.05]
        # Si k tombe dans cet intervalle, on le repousse
        mask_ambiguous = (factors >= 0.95) & (factors <= 1.05)
        n_ambiguous = mask_ambiguous.sum()

        if n_ambiguous > 0:
            # Remplace les valeurs ambigu√´s par des valeurs claires
            # 50% en dessous de 0.95, 50% au-dessus de 1.05
            new_factors = np.where(
                rng.random(n_ambiguous) < 0.5,
                rng.uniform(k_min, 0.95, size=n_ambiguous),
                rng.uniform(1.05, k_max, size=n_ambiguous),
            )
            factors[mask_ambiguous] = new_factors

        # Applique le scaling
        # Broadcasting: (n_samples,) √ó (n_samples, dim)
        states = states_valid * factors[:, np.newaxis]

    # === STRAT√âGIE NOISE ===
    elif strategy == "noise":
        # G√©n√®re des √©tats valides
        states_valid = generate_valid_states(
            n_samples=n_samples,
            dim=dim,
            strategy="random",
            seed=int(rng.integers(0, int(1e9))),
        )

        # G√©n√®re du bruit complexe
        noise_real = rng.normal(0, noise_level, size=(n_samples, dim))
        noise_imag = rng.normal(0, noise_level, size=(n_samples, dim))
        noise = noise_real + 1j * noise_imag

        # Ajoute le bruit (sans renormaliser !)
        states = states_valid + noise

    # === STRAT√âGIE DIRECT ===
    elif strategy == "direct":
        # G√©n√®re directement sans normaliser
        real_parts = rng.normal(0, 1, size=(n_samples, dim))
        imag_parts = rng.normal(0, 1, size=(n_samples, dim))
        states = real_parts + 1j * imag_parts

        # Applique un scaling al√©atoire pour varier ||œà||¬≤
        scale_factors = rng.uniform(0.1, 3.0, size=n_samples)
        states = states * scale_factors[:, np.newaxis]

    # === STRAT√âGIE MIXED ===
    elif strategy == "mixed":
        states = np.zeros((n_samples, dim), dtype=complex)

        # R√©partition des sous-strat√©gies
        n_extreme = int(n_samples * extreme_prob)
        n_remaining = n_samples - n_extreme

        # Distribution du reste entre scaling, noise, direct
        n_scaling = n_remaining // 3
        n_noise = n_remaining // 3
        n_direct = n_remaining - n_scaling - n_noise

        idx = 0

        # 1. Cas extr√™mes
        if n_extreme > 0:
            states_extreme = _generate_extreme_states(n_extreme, dim, rng)
            states[idx : idx + n_extreme] = states_extreme
            idx += n_extreme

        # 2. Scaling
        if n_scaling > 0:
            states_scaling = generate_invalid_states(
                n_scaling,
                dim,
                strategy="scaling",
                scale_range=scale_range,
                seed=int(rng.integers(0, int(1e9))),
            )
            states[idx : idx + n_scaling] = states_scaling
            idx += n_scaling

        # 3. Noise
        if n_noise > 0:
            states_noise = generate_invalid_states(
                n_noise,
                dim,
                strategy="noise",
                noise_level=noise_level,
                seed=int(rng.integers(0, int(1e9))),
            )
            states[idx : idx + n_noise] = states_noise
            idx += n_noise

        # 4. Direct
        if n_direct > 0:
            states_direct = generate_invalid_states(
                n_direct, dim, strategy="direct", seed=int(rng.integers(0, int(1e9)))
            )
            states[idx : idx + n_direct] = states_direct

        # M√©lange al√©atoirement
        rng.shuffle(states)

    # === V√âRIFICATION FINALE ===
    # S'assure qu'aucun √©tat n'est accidentellement normalis√©
    norms_squared = np.sum(np.abs(states) ** 2, axis=1)
    accidentally_normalized = np.isclose(norms_squared, 1.0, atol=1e-4)

    if accidentally_normalized.any():
        # Rescale l√©g√®rement ces √©tats
        indices = np.where(accidentally_normalized)[0]
        for idx in indices:
            # Multiplie par un facteur al√©atoire loin de 1
            factor = rng.choice([0.7, 0.8, 1.2, 1.3])
            states[idx] *= factor

    return states


def _generate_extreme_states(n_samples: int, dim: int, rng) -> np.ndarray:

    states = np.zeros((n_samples, dim), dtype=complex)

    for i in range(n_samples):
        case_type = rng.choice(["null", "huge", "unbalanced"])

        if case_type == "null":
            # √âtat quasi-nul
            states[i] = rng.normal(0, 0.01, dim) + 1j * rng.normal(0, 0.01, dim)

        elif case_type == "huge":
            # √âtat tr√®s grand
            states[i] = rng.normal(10, 5, dim) + 1j * rng.normal(10, 5, dim)

        elif case_type == "unbalanced":
            # Une composante √©norme, les autres petites
            dominant_idx = rng.integers(0, dim)
            states[i] = rng.normal(0, 0.1, dim) + 1j * rng.normal(0, 0.1, dim)
            states[i, dominant_idx] = rng.uniform(50, 100) + 1j * rng.uniform(50, 100)

    return states


def get_invalid_strategy_info() -> dict:

    info = {
        "scaling": (
            "Multiplie des √©tats valides par un facteur k ‚â† 1. "
            "Contr√¥le: k ‚àà [k_min, k_max] en √©vitant [0.95, 1.05]. "
            "Produit: ||œà||¬≤ = k¬≤. "
            "Recommand√© pour usage g√©n√©ral."
        ),
        "noise": (
            "Ajoute du bruit √† des √©tats valides sans renormaliser. "
            "Param√®tre noise_level contr√¥le l'intensit√©. "
            "Produit: √©tats 'presque valides' (utile pour robustesse)."
        ),
        "direct": (
            "G√©n√®re directement des coefficients sans normalisation. "
            "Produit: large distribution de ||œà||¬≤. "
            "Bonne diversit√©."
        ),
        "mixed": (
            "Combine scaling, noise, direct + cas extr√™mes. "
            "Param√®tre extreme_prob contr√¥le % de outliers. "
            "Produit: dataset tr√®s diversifi√©. "
            "Recommand√© pour dataset final."
        ),
    }
    return info


def print_invalid_strategy_info():

    info = get_invalid_strategy_info()

    print("=" * 70)
    print("STRAT√âGIES DE G√âN√âRATION D'√âTATS INVALIDES")
    print("=" * 70)

    for strategy, description in info.items():
        print(f"\nüìå {strategy.upper()}")
        print(f"   {description}")

    print("\n" + "=" * 70)


# ============================================================================
# CR√âATION DU DATASET COMPLET
# ============================================================================

import pandas as pd


def create_dataset(
    n_valid: int,
    n_invalid: int,
    dim: int,
    valid_strategy: str = "random",
    invalid_strategy: str = "mixed",
    valid_kwargs: Optional[dict] = None,
    invalid_kwargs: Optional[dict] = None,
    seed: Optional[int] = None,
    shuffle: bool = True,
) -> pd.DataFrame:

    # Initialisation du g√©n√©rateur al√©atoire
    rng = np.random.default_rng(seed)

    # Graines pour les sous-g√©n√©rateurs (pour reproductibilit√©)
    # Utilise des bornes enti√®res et convertit le r√©sultat en int Python
    seed_valid = int(rng.integers(0, 1_000_000_000)) if seed is not None else None
    seed_invalid = int(rng.integers(0, 1_000_000_000)) if seed is not None else None

    # === G√âN√âRATION DES √âTATS VALIDES ===
    print(f"G√©n√©ration de {n_valid} √©tats valides (strat√©gie: {valid_strategy})...")

    valid_kwargs = valid_kwargs or {}
    states_valid = generate_valid_states(
        n_samples=n_valid,
        dim=dim,
        strategy=valid_strategy,
        seed=seed_valid,
        **valid_kwargs,
    )

    # === G√âN√âRATION DES √âTATS INVALIDES ===
    print(
        f"G√©n√©ration de {n_invalid} √©tats invalides (strat√©gie: {invalid_strategy})..."
    )

    invalid_kwargs = invalid_kwargs or {}
    states_invalid = generate_invalid_states(
        n_samples=n_invalid,
        dim=dim,
        strategy=invalid_strategy,
        seed=seed_invalid,
        **invalid_kwargs,
    )

    # === CONSTRUCTION DU DATAFRAME ===
    print("Construction du DataFrame...")

    # Combine les deux ensembles
    all_states = np.vstack([states_valid, states_invalid])
    n_total = n_valid + n_invalid

    # Cr√©e les labels
    labels = np.concatenate(
        [
            np.ones(n_valid, dtype=int),  # 1 pour valides
            np.zeros(n_invalid, dtype=int),  # 0 pour invalides
        ]
    )

    # Calcule les normes¬≤
    norms_squared = np.sum(np.abs(all_states) ** 2, axis=1)

    # Construit le dictionnaire de donn√©es
    data_dict = {}

    # Colonne state_id
    data_dict["state_id"] = np.arange(n_total)

    # Assure que all_states est un ndarray de dtype complex pour des op√©rations s√ªres
    all_states = np.asarray(all_states, dtype=complex)

    # Colonnes c{i}_real et c{i}_imag (utilise les fonctions numpy pour √©viter les probl√®mes de typage)
    for i in range(dim):
        data_dict[f"c{i}_real"] = np.real(all_states[:, i])
        data_dict[f"c{i}_imag"] = np.imag(all_states[:, i])

    # Colonne norm_squared
    data_dict["norm_squared"] = norms_squared

    # Colonne is_valid (target)
    data_dict["is_valid"] = labels

    # Cr√©e le DataFrame
    df = pd.DataFrame(data_dict)

    # === M√âLANGE (SHUFFLE) ===
    if shuffle:
        print("M√©lange du dataset...")
        df = df.sample(frac=1.0, random_state=seed).reset_index(drop=True)
        # R√©attribue les state_id apr√®s shuffle
        df["state_id"] = np.arange(len(df))

    # === STATISTIQUES ===
    print("\n" + "=" * 70)
    print("DATASET CR√â√â")
    print("=" * 70)
    print(f"Shape: {df.shape}")
    print(f"Dimension: {dim}")
    print(f"√âtats valides: {n_valid} ({n_valid/n_total*100:.1f}%)")
    print(f"√âtats invalides: {n_invalid} ({n_invalid/n_total*100:.1f}%)")
    print(f"\n Distribution de is_valid:")
    print(df["is_valid"].value_counts().sort_index())

    print(f"\n Statistiques de norm_squared:")
    print(df["norm_squared"].describe())

    print("=" * 70)

    return df


def save_dataset(
    df: pd.DataFrame,
    filename: str = "quantum_states_dataset.csv",
    data_dir: str = "data/processed",
) -> Path:

    # Cr√©e le dossier si n√©cessaire
    data_path = Path(data_dir)
    data_path.mkdir(parents=True, exist_ok=True)

    # Chemin complet
    filepath = data_path / filename

    # Sauvegarde
    df.to_csv(filepath, index=False)

    # Taille du fichier
    file_size = filepath.stat().st_size / 1024  # en KB

    print(f"\n Dataset sauvegard√©:")
    print(f"   Chemin: {filepath}")
    print(f"   Taille: {file_size:.2f} KB")
    print(f"   Lignes: {len(df)}")
    print(f"   Colonnes: {len(df.columns)}")

    return filepath


def load_dataset(
    filename: str = "quantum_states_dataset.csv", data_dir: str = "data/processed"
) -> pd.DataFrame:

    filepath = Path(data_dir) / filename

    if not filepath.exists():
        raise FileNotFoundError(
            f"Le fichier {filepath} n'existe pas. "
            f"Utilisez create_dataset() puis save_dataset() d'abord."
        )

    df = pd.read_csv(filepath)

    print(f"Dataset charg√© depuis {filepath}")
    print(f"Shape: {df.shape}")

    return df


def get_dataset_info(df: pd.DataFrame) -> dict:

    # D√©duit la dimension
    # Colonnes: state_id, c0_real, c0_imag, ..., c{d-1}_real, c{d-1}_imag, norm_squared, is_valid
    # Nombre de colonnes c{i}_* = 2*dim
    n_c_columns = len([col for col in df.columns if col.startswith("c")])
    dim = n_c_columns // 2

    info = {
        "n_samples": len(df),
        "n_features": n_c_columns + 1,  # +1 pour norm_squared
        "dim": dim,
        "n_valid": (df["is_valid"] == 1).sum(),
        "n_invalid": (df["is_valid"] == 0).sum(),
        "balance_ratio": (df["is_valid"] == 1).sum() / (df["is_valid"] == 0).sum(),
        "norm_stats": df["norm_squared"].describe().to_dict(),
    }

    return info


def print_dataset_info(df: pd.DataFrame):

    info = get_dataset_info(df)

    print("=" * 70)
    print("INFORMATIONS SUR LE DATASET")
    print("=" * 70)
    print(f"\n Taille:")
    print(f"   √âchantillons: {info['n_samples']}")
    print(f"   Features: {info['n_features']}")
    print(f"   Dimension: {info['dim']}")

    print(f"\n Distribution des classes:")
    print(
        f"   Valides (1): {info['n_valid']} ({info['n_valid']/info['n_samples']*100:.1f}%)"
    )
    print(
        f"   Invalides (0): {info['n_invalid']} ({info['n_invalid']/info['n_samples']*100:.1f}%)"
    )
    print(f"   Ratio: {info['balance_ratio']:.3f}")

    if abs(info["balance_ratio"] - 1.0) < 0.05:
        print(f"    Dataset bien √©quilibr√©")
    else:
        print(f"     Dataset d√©s√©quilibr√© (id√©al: ratio ‚âà 1.0)")

    print(f"\n Statistiques de norm_squared:")
    for key, value in info["norm_stats"].items():
        print(f"   {key:8s}: {value:.6f}")

    print("=" * 70)


if __name__ == "__main__":
    # Ce bloc s'ex√©cute uniquement si on lance : python src/data_generation.py

    print("Test du module data_generation.py\n")

    # Affiche les strat√©gies disponibles
    print_strategy_info()

    # Test des 3 strat√©gies
    print("\n" + "=" * 70)
    print("TESTS DE G√âN√âRATION")
    print("=" * 70)

    dim = 3
    n_samples = 5

    for strategy in ["random", "dirichlet", "basis"]:
        print(f"\n--- Strat√©gie : {strategy} ---")

        states = generate_valid_states(
            n_samples=n_samples, dim=dim, strategy=strategy, seed=42
        )

        print(f"Forme du tableau : {states.shape}")
        print(f"Type de donn√©es : {states.dtype}")

        # V√©rification
        all_valid, norms = verify_normalization(states)
        print(f"Tous les √©tats sont normalis√©s ? {all_valid}")
        print(f"Normes au carr√© : {norms}")

        # Affiche les 2 premiers √©tats
        print(f"\n2 premiers √©tats :")
        for i in range(min(2, n_samples)):
            print(f"  √âtat {i} : {states[i]}")
            print(f"    ||œà||¬≤ = {norms[i]:.10f}")
